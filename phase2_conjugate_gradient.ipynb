{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "phase2_conjugate_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidkuernsteiner/numerical_optimization/blob/main/phase2_conjugate_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9NOe0yxcZfu"
      },
      "source": [
        "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTWMMYGucZfx"
      },
      "source": [
        "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZrrkIhScZfy"
      },
      "source": [
        "<i>Group 1<br>\n",
        "Participants information in alphabetical order</i>\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th style = \"text-align: left\">#</th>\n",
        "    <th style = \"text-align: left\">Name</th>\n",
        "    <th style = \"text-align: left\">Lastname</th>\n",
        "    <th style = \"text-align: left\">Matr Number</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">1</td>\n",
        "    <td style = \"text-align: left\">David</td>\n",
        "    <td style = \"text-align: left\">KÃ¼rnsteiner</td>\n",
        "    <td style = \"text-align: left\">11820336</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">2</td>\n",
        "    <td style = \"text-align: left\">Christian</td>\n",
        "    <td style = \"text-align: left\">Peinthor</td>\n",
        "    <td style = \"text-align: left\">11815592</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">3</td>\n",
        "    <td style = \"text-align: left\">Elias</td>\n",
        "    <td style = \"text-align: left\">Ramoser</td>\n",
        "    <td style = \"text-align: left\">11918558</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td style = \"text-align: left\">4</td>\n",
        "    <td style = \"text-align: left\">Georg</td>\n",
        "    <td style = \"text-align: left\">Storz</td>\n",
        "    <td style = \"text-align: left\">11918811</td>\n",
        "    </tr>\n",
        "  \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egf-RyNlcZfz"
      },
      "source": [
        "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
        "<i>All points x are represented as numpy arrays. Function f returns a scalar with grad_f and hessian_f returning numpy arrays.<i>\n",
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
        "<i>Describe how to install additional packages, if you have some, here</i>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5rKWVctcZf0"
      },
      "source": [
        "import numpy as np\n",
        "import scipy \n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy.linalg import *\n",
        "from sklearn.datasets import make_spd_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opT-uwtJcZf0"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
        "<i>Function returns True if the gradient of f at xk relative to x0 is smaller than parameter tol. Additionally there is an upper bound for iterations to stop non converging algorithms.<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGU8MquEcZf1"
      },
      "source": [
        "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
        "  if i > max_iter: \n",
        "    return True\n",
        "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
        "    return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBEAwLHcZf2"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
        "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
        "This is additional task, which can earn you several points.<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90K3ffG0dkFl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Qy9Ci0cZf2"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
        "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
        "This is additional task, which can earn you several points.<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zsxIWEAcZf3"
      },
      "source": [
        "#your function for stabilising goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpA1FRYDcZf4"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
        "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
        "This is additional task, which can earn you several points.<i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QTrMirmcZf4"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
        "<i>linear_solve() provides a way to solve linear systems of equations using a LU-factorization of A and subsequent forward and backward substitution as described in the book. This solver proves to be quite unstable though in practical applications. We therefore use the numpy implementation of solve(). <i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWSeLfhAcZf4"
      },
      "source": [
        "def forward_substitution(L, b):\n",
        "\n",
        "    n = L.shape[0]\n",
        "\n",
        "    y = np.zeros_like(b, dtype=np.double);\n",
        "\n",
        "    y[0] = b[0] / L[0, 0]\n",
        "\n",
        "    for i in range(1, n):\n",
        "        y[i] = (b[i] - np.dot(L[i,:i], y[:i])) / L[i,i]\n",
        "        \n",
        "    return y\n",
        "\n",
        "def back_substitution(U, y):\n",
        "    \n",
        "    n = U.shape[0]\n",
        "\n",
        "    x = np.zeros_like(y, dtype=np.double);\n",
        "\n",
        "    x[-1] = y[-1] / U[-1, -1]\n",
        "    \n",
        "    for i in range(n-2, -1, -1):\n",
        "        x[i] = (y[i] - np.dot(U[i,i:], x[i:])) / U[i,i]\n",
        "        \n",
        "    return x\n",
        "\n",
        "def linear_solve(A, b):\n",
        "    \n",
        "    P, L, U = scipy.linalg.lu(A)\n",
        "    \n",
        "    y = forward_substitution(L, P @ b)\n",
        "    \n",
        "    return back_substitution(U, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnWsNJuicZf5"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
        "<i>Following functions are wrapper functions that provide approximations of the gradient and hessian of f using the forward-difference approach as described in the book. <i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJcCQY8cZf5"
      },
      "source": [
        "def e_i(size, index):\n",
        "  arr = np.zeros(size)\n",
        "  arr[index] = 1.0\n",
        "  return arr\n",
        "\n",
        "def approx_grad(f, e=1.1*10**-8):\n",
        "  def grad_f(x):\n",
        "    if x.size == 1:\n",
        "      return (f(x + e) - f(x)) / e\n",
        "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
        "  return grad_f\n",
        "\n",
        "def approx_hessian(f, e=1.1*10**-8):\n",
        "  def hessian_f(x):\n",
        "    if x.size == 1:\n",
        "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
        "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
        "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
        "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
        "  return hessian_f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkKtvzncZf5"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
        "<i>The class Problem() provides an object to generate and set up quadratic and non some non quadratic test problems for the algorithms with.<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqyBMX7DcZf6"
      },
      "source": [
        "class Problem():\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.f = None\n",
        "    self.grad_f = None\n",
        "    self.hessian_f = None\n",
        "    self.min_x = None\n",
        "\n",
        "  def quadratic(self, n_dim, rseed):\n",
        "\n",
        "    rng = np.random.RandomState(rseed)\n",
        "    A = make_spd_matrix(n_dim, random_state=rseed)\n",
        "    x = rng.randint(-10, 10, n_dim)\n",
        "    b = A @ x\n",
        "\n",
        "    def f(x):\n",
        "      return 0.5 * x.T @ A @ x - b @ x\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return A @ x - b\n",
        "\n",
        "    def hessian_f(x):\n",
        "      return A\n",
        "    \n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian_f = hessian_f\n",
        "    self.min_x = x\n",
        "    self.A = A\n",
        "    self.b = b\n",
        "\n",
        "  def rosenbrock(self):\n",
        "\n",
        "    def f(x):\n",
        "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "    def grad_f(x):\n",
        "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
        "           200*(x[1] - x[0]**2)])\n",
        "\n",
        "    def hessian_f(x):\n",
        "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
        "           [-400*x[0], 200]])\n",
        "      \n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian_f = hessian_f\n",
        "    self.min_x = np.array([1,1])\n",
        "\n",
        "  def himmelblau(self):\n",
        "\n",
        "    def f(x):\n",
        "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
        "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
        "    \n",
        "    def hessian_f(x):\n",
        "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
        "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
        "      \n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian_f = hessian_f\n",
        "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
        "\n",
        "  def poly_1(self):\n",
        "\n",
        "    def f(x):\n",
        "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return (x - 7) * (x - 5) * (x - 3)\n",
        "    \n",
        "    def hessian_f(x):\n",
        "      return 3 * x**2 - 30 * x + 71\n",
        "\n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian_f = hessian_f\n",
        "    self.min_x = np.array([[3],[5],[7]])\n",
        "\n",
        "  def poly_2(self):\n",
        "\n",
        "    def f(x):\n",
        "      return (x**2 * (x**2 - 16*x + 40)) / 4\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return x * (x - 2) * (x - 10)\n",
        "    \n",
        "    def hessian_f(x):\n",
        "      return 3 * x**2 - 24 * x + 20\n",
        "\n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian_f = hessian_f\n",
        "    self.min_x = np.array([[0],[2],[10]])\n",
        "\n",
        "  def poly_3(self):\n",
        "\n",
        "    def f(x):\n",
        "      return (x * (3 * x**3 - 64 * x**2 + 414 * x - 648)) / 12\n",
        "    \n",
        "    def grad_f(x):\n",
        "      return (x - 1) * (x - 6) * (x - 9)\n",
        "    \n",
        "    def hessian_f(x):\n",
        "      return 3 * x**2 - 32 * x + 69\n",
        "\n",
        "    self.f = f\n",
        "    self.grad_f = grad_f\n",
        "    self.hessian_f = hessian_f\n",
        "    self.min_x = np.array([[1],[6],[9]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EOhfs8acZf6"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
        "<i>backtracking_alpha() implements the backtracking line search to find a suitable step length as described in the book. FR() implements the Fletcher Reeves nonlinear conjugate gradient method.<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b1zjSs3cZf6"
      },
      "source": [
        "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
        "\n",
        "  alpha = alpha0\n",
        "\n",
        "  while not f(xk + alpha * pk) <= (f(xk) + c * alpha * grad_f(xk).T @ pk):\n",
        "    alpha *= rho\n",
        "  \n",
        "  return alpha\n",
        "\n",
        "def FR(x0, f, grad_f=None):\n",
        "\n",
        "  conv_tol = 1e-8\n",
        "  if grad_f == None:\n",
        "    grad_f = approx_grad(f)\n",
        "    conv_tol = 1e-6\n",
        "  i = 0\n",
        "  xk = x0\n",
        "  pk = -grad_f(xk)\n",
        "\n",
        "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
        "\n",
        "    xk1 = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
        "    beta = (grad_f(xk1) @ grad_f(xk1)) / (grad_f(xk) @ grad_f(xk))\n",
        "    pk = -grad_f(xk1) + beta * pk\n",
        "    xk = xk1\n",
        "    i += 1\n",
        " \n",
        "  print(f\"\\nsearch terminated at iteration {i} | result: {xk}\")\n",
        "  return xk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF6N5VLVcZf7"
      },
      "source": [
        "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
        "<i>Place for additional comments and argumentation<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXevbYiwcZf7"
      },
      "source": [
        "rseed = [1,4,6,7,8]\n",
        "quadratic_probs = []\n",
        "for i in range(5):\n",
        "  prob = Problem()\n",
        "  prob.quadratic(10, rseed[i])\n",
        "  quadratic_probs.append(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1TBCRNucZf7"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
        "<p><b>Note:</b> After every test print out the resulsts. \n",
        "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
        "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
        "<p><i>Place for your additional comments and argumentation<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfgMEOMLcZf7"
      },
      "source": [
        "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    --------------------------------------------------------------------------------------------------------------\n",
        "    x_0: numpy 1D array, corresponds to initial point\n",
        "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
        "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
        "    --------------------------------------------------------------------------------------------------------------\n",
        "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
        "                                      **args\n",
        "       Function f returns a scalar output.\n",
        "    --------------------------------------------------------------------------------------------------------------\n",
        "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
        "                                         function f,\n",
        "                                         args (which are submitted, because you might need\n",
        "                                              to call f(x,**args) inside your gradient function implementation). \n",
        "          Function grad approximates gradient at given point and returns a 1d np array.\n",
        "    --------------------------------------------------------------------------------------------------------------\n",
        "    args: dictionary, additional (except of x) arguments to function f\n",
        "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f'Initial x is :\\t\\t{x_0}')\n",
        "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
        "    print(f'Approximated x is :\\t{x_appr}')\n",
        "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
        "    f_opt = f(x_optimal,**args)\n",
        "    f_appr = f(x_appr,**args)\n",
        "    print(f'Function value in optimal point:\\t{f_opt}')\n",
        "    print(f'Function value in approximated point:   {f_appr}')\n",
        "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
        "    print(f'Gradient approximation in optimal point is:\\n{grad(f,x_optimal,args)}\\n')\n",
        "    grad_appr = grad(f,x_appr,args)\n",
        "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
        "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSmyQfOLcZf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad581373-330e-4262-e22a-c518c3998417"
      },
      "source": [
        "for i, prob in enumerate(quadratic_probs):\n",
        "  print(f\"Problem {i+1}: \")\n",
        "  print(\"approximated gradient: \")\n",
        "  FR(np.zeros(10), prob.f)\n",
        "  print(\"\\n exact gradient: \")\n",
        "  FR(np.zeros(10), prob.f, prob.grad_f)\n",
        "  print(f\"\\n actual minimum: {prob.min_x}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem 1: \n",
            "approximated gradient: \n",
            "\n",
            "search terminated at iteration 654 | result: [ -5.00003438   1.00002037   1.99998906  -2.00000149  -1.00000396\n",
            "   0.99998883  -5.00000031   4.99999505 -10.00000355   6.00000115]\n",
            "\n",
            " exact gradient: \n",
            "\n",
            "search terminated at iteration 2930 | result: [-5.0000002   1.00000004  2.00000014 -2.00000001 -0.99999999  1.00000001\n",
            " -5.00000001  4.99999997 -9.99999989  6.00000011]\n",
            "\n",
            " actual minimum: [ -5   1   2  -2  -1   1  -5   5 -10   6]\n",
            "\n",
            "Problem 2: \n",
            "approximated gradient: \n",
            "\n",
            "search terminated at iteration 202 | result: [ 4.00007019 -4.99997688 -8.99996437 -1.99993588 -1.99998155  7.99998998\n",
            " -0.99994231 -3.00000223  3.00000139 -1.99999319]\n",
            "\n",
            " exact gradient: \n",
            "\n",
            "search terminated at iteration 868 | result: [ 4.00000017 -5.00000008 -8.99999984 -2.00000021 -2.00000035  8.\n",
            " -1.00000015 -2.9999998   2.99999965 -2.00000025]\n",
            "\n",
            " actual minimum: [ 4 -5 -9 -2 -2  8 -1 -3  3 -2]\n",
            "\n",
            "Problem 3: \n",
            "approximated gradient: \n",
            "\n",
            "search terminated at iteration 1226 | result: [ 1.28542515e-06 -9.99979279e-01 -7.00000467e+00 -5.41300737e-07\n",
            "  3.00000206e+00  5.00000469e+00  2.11070375e-05  5.99999225e+00\n",
            " -8.99999391e+00  1.00003025e+00]\n",
            "\n",
            " exact gradient: \n",
            "\n",
            "search terminated at iteration 801 | result: [-3.13974964e-07 -9.99999839e-01 -6.99999997e+00  2.22101395e-07\n",
            "  3.00000014e+00  5.00000012e+00  6.32919951e-08  6.00000007e+00\n",
            " -9.00000020e+00  9.99999958e-01]\n",
            "\n",
            " actual minimum: [ 0 -1 -7  0  3  5  0  6 -9  1]\n",
            "\n",
            "Problem 4: \n",
            "approximated gradient: \n",
            "\n",
            "search terminated at iteration 2809 | result: [ 4.99999442e+00 -6.00001356e+00 -7.00003001e+00  8.99998382e+00\n",
            " -3.00001862e+00  4.00000136e+00 -2.00002591e+00  4.00000293e+00\n",
            " -4.50587567e-05 -2.00002389e+00]\n",
            "\n",
            " exact gradient: \n",
            "\n",
            "search terminated at iteration 5001 | result: [ 4.99999983e+00 -5.99999996e+00 -7.00000038e+00  8.99999990e+00\n",
            " -2.99999999e+00  4.00000007e+00 -2.00000030e+00  4.00000014e+00\n",
            " -2.01001012e-07 -2.00000008e+00]\n",
            "\n",
            " actual minimum: [ 5 -6 -7  9 -3  4 -2  4  0 -2]\n",
            "\n",
            "Problem 5: \n",
            "approximated gradient: \n",
            "\n",
            "search terminated at iteration 237 | result: [-6.99997342  6.99996826 -1.00001791 -5.00004693 -2.00001951  9.00003098\n",
            " -2.00000242  6.00005733  2.99994874  6.99996473]\n",
            "\n",
            " exact gradient: \n",
            "\n",
            "search terminated at iteration 868 | result: [-6.9999995   6.99999919 -1.00000019 -5.00000033 -2.00000022  9.00000019\n",
            " -2.00000065  6.00000068  2.99999993  6.99999967]\n",
            "\n",
            " actual minimum: [-7  7 -1 -5 -2  9 -2  6  3  7]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5rkOOgOcZf8"
      },
      "source": [
        "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SW5-O0ZcZf8"
      },
      "source": [
        "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
        "<i>Place for additional comments and argumentation<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN0W_bYycZf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae0a373-f963-48dd-a30f-3dbaa24fe36d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem rosenbrock: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 548 | result: [1.00000649 1.00001329]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 1037 | result: [0.99999979 0.99999958]\n",
            "\n",
            "actual minimum: [1 1]\n",
            "\n",
            "Problem himmelblau: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 523 | result: [2.99999955 2.00000087]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 769 | result: [-2.80511808  3.13131252]\n",
            "\n",
            "actual minimum: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Problem poly_1: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 42 | result: [7.00000184]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 60 | result: [6.99999999]\n",
            "\n",
            "actual minimum: [[3]\n",
            " [5]\n",
            " [7]]\n",
            "\n",
            "Problem poly_2: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 338 | result: [9.9999998]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 1444 | result: [10.]\n",
            "\n",
            "actual minimum: [[ 0]\n",
            " [ 2]\n",
            " [10]]\n",
            "\n",
            "Problem poly_3: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 306 | result: [1.0000003]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 514 | result: [1.]\n",
            "\n",
            "actual minimum: [[1]\n",
            " [6]\n",
            " [9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxJIUhehcZf9"
      },
      "source": [
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
        "<p><i>Place for your additional comments and argumentation<i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSEog4wqcZf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ccea1c-1d3c-4690-d47c-51a8cea0363c"
      },
      "source": [
        "prob = Problem()\n",
        "prob.rosenbrock()\n",
        "print(f\"Problem rosenbrock: \\n\")\n",
        "print(\"approximate gradient: \")\n",
        "FR(np.array([1.2,1.2]), prob.f)\n",
        "print(\"\\nexact gradient: \")\n",
        "FR(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minimum: {prob.min_x}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.himmelblau()\n",
        "print(f\"\\nProblem himmelblau: \\n\")\n",
        "print(\"approximate gradient: \")\n",
        "FR(np.array([0,0]), prob.f)\n",
        "print(\"\\nexact gradient: \")\n",
        "FR(np.array([0,0]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minimum: {prob.min_x}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.poly_1()\n",
        "print(f\"\\nProblem poly_1: \\n\")\n",
        "print(\"approximate gradient: \")\n",
        "FR(np.array([2]), prob.f)\n",
        "print(\"\\nexact gradient: \")\n",
        "FR(np.array([2]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minimum: {prob.min_x}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.poly_2()\n",
        "print(f\"\\nProblem poly_2: \\n\")\n",
        "print(\"approximate gradient: \")\n",
        "FR(np.array([1]), prob.f)\n",
        "print(\"\\nexact gradient: \")\n",
        "FR(np.array([1]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minimum: {prob.min_x}\")\n",
        "\n",
        "prob = Problem()\n",
        "prob.poly_3()\n",
        "print(f\"\\nProblem poly_3: \\n\")\n",
        "print(\"approximate gradient: \")\n",
        "FR(np.array([7]), prob.f)\n",
        "print(\"\\nexact gradient: \")\n",
        "FR(np.array([7]), prob.f, prob.grad_f)\n",
        "print(f\"\\nactual minimum: {prob.min_x}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem rosenbrock: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 548 | result: [1.00000649 1.00001329]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 1037 | result: [0.99999979 0.99999958]\n",
            "\n",
            "actual minimum: [1 1]\n",
            "\n",
            "Problem himmelblau: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 523 | result: [2.99999955 2.00000087]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 769 | result: [-2.80511808  3.13131252]\n",
            "\n",
            "actual minimum: [[ 3.        2.      ]\n",
            " [-2.805118  3.131312]\n",
            " [-3.77931  -3.283186]\n",
            " [ 3.584428 -1.848126]]\n",
            "\n",
            "Problem poly_1: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 42 | result: [7.00000184]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 60 | result: [6.99999999]\n",
            "\n",
            "actual minimum: [[3]\n",
            " [5]\n",
            " [7]]\n",
            "\n",
            "Problem poly_2: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 338 | result: [9.9999998]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 1444 | result: [10.]\n",
            "\n",
            "actual minimum: [[ 0]\n",
            " [ 2]\n",
            " [10]]\n",
            "\n",
            "Problem poly_3: \n",
            "\n",
            "approximate gradient: \n",
            "\n",
            "search terminated at iteration 306 | result: [1.0000003]\n",
            "\n",
            "exact gradient: \n",
            "\n",
            "search terminated at iteration 514 | result: [1.]\n",
            "\n",
            "actual minimum: [[1]\n",
            " [6]\n",
            " [9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFNIPpjxcZf9"
      },
      "source": [
        "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oU43ESvcZf9"
      },
      "source": [
        "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
        "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
        "Template should include sceletons for:<ul>\n",
        "    <li>custom function to optimise over </li> \n",
        "    <li>values initialisation to submit into otimising algorithm </li> \n",
        "    <li>optimiser function call</li> \n",
        "    <li>report print out call</li> </ul><br>\n",
        "Provide descriptions and comments."
      ]
    }
  ]
}
