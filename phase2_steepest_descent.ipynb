{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidkuernsteiner/numerical_optimization/blob/main/phase2_steepest_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9NOe0yxcZfu"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTWMMYGucZfx"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZrrkIhScZfy"
   },
   "source": [
    "<i>Group 1<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">David</td>\n",
    "    <td style = \"text-align: left\">KÃ¼rnsteiner</td>\n",
    "    <td style = \"text-align: left\">11820336</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Elias</td>\n",
    "    <td style = \"text-align: left\">Ramoser</td>\n",
    "    <td style = \"text-align: left\">11918558</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Georg</td>\n",
    "    <td style = \"text-align: left\">Storz</td>\n",
    "    <td style = \"text-align: left\">11918811</td>\n",
    "    </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egf-RyNlcZfz"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<i>All points x are represented as numpy arrays. Function f returns a scalar with grad_f and hessian_f returning numpy arrays.<i>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H5rKWVctcZf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *\n",
    "from sklearn.datasets import make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opT-uwtJcZf0"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Function returns True if the gradient of f at xk relative to x0 is smaller than parameter tol. Additionally there is an upper bound for iterations to stop non converging algorithms.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZGU8MquEcZf1"
   },
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
    "  if i > max_iter: \n",
    "    return True\n",
    "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBEAwLHcZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90K3ffG0dkFl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Qy9Ci0cZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9zsxIWEAcZf3"
   },
   "outputs": [],
   "source": [
    "#your function for stabilising goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpA1FRYDcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTrMirmcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>linear_solve() provides a way to solve linear systems of equations using a LU-factorization of A and subsequent forward and backward substitution as described in the book. This solver proves to be quite unstable though in practical applications. We therefore use the numpy implementation of solve(). <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wWSeLfhAcZf4"
   },
   "outputs": [],
   "source": [
    "def forward_substitution(L, b):\n",
    "\n",
    "    n = L.shape[0]\n",
    "\n",
    "    y = np.zeros_like(b, dtype=np.double);\n",
    "\n",
    "    y[0] = b[0] / L[0, 0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        y[i] = (b[i] - np.dot(L[i,:i], y[:i])) / L[i,i]\n",
    "        \n",
    "    return y\n",
    "\n",
    "def back_substitution(U, y):\n",
    "    \n",
    "    n = U.shape[0]\n",
    "\n",
    "    x = np.zeros_like(y, dtype=np.double);\n",
    "\n",
    "    x[-1] = y[-1] / U[-1, -1]\n",
    "    \n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = (y[i] - np.dot(U[i,i:], x[i:])) / U[i,i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def linear_solve(A, b):\n",
    "    \n",
    "    P, L, U = scipy.linalg.lu(A)\n",
    "    \n",
    "    y = forward_substitution(L, P @ b)\n",
    "    \n",
    "    return back_substitution(U, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWsNJuicZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Following functions are wrapper functions that provide approximations of the gradient and hessian of f using the forward-difference approach as described in the book. <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1tJcCQY8cZf5"
   },
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "  arr = np.zeros(size)\n",
    "  arr[index] = 1.0\n",
    "  return arr\n",
    "\n",
    "def approx_grad(f, e=1.1*10**-8):\n",
    "  def grad_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + e) - f(x)) / e\n",
    "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "  return grad_f\n",
    "\n",
    "def approx_hessian(f, e=1.1*10**-8):\n",
    "  def hessian_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
    "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
    "  return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkKtvzncZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>The class Problem() provides an object to generate and set up quadratic and non some non quadratic test problems for the algorithms with.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lqyBMX7DcZf6"
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    self.f = None\n",
    "    self.grad_f = None\n",
    "    self.hessian_f = None\n",
    "    self.min_x = None\n",
    "\n",
    "  def quadratic(self, n_dim, rseed):\n",
    "\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    A = make_spd_matrix(n_dim, random_state=rseed)\n",
    "    x = rng.randint(-10, 10, n_dim)\n",
    "    b = A @ x\n",
    "\n",
    "    def f(x):\n",
    "      return 0.5 * x.T @ A @ x - b @ x\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return A @ x - b\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return A\n",
    "    \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = x\n",
    "    self.A = A\n",
    "    self.b = b\n",
    "\n",
    "  def rosenbrock(self):\n",
    "\n",
    "    def f(x):\n",
    "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(x):\n",
    "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "           200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
    "           [-400*x[0], 200]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([1,1])\n",
    "\n",
    "  def himmelblau(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
    "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
    "\n",
    "  def poly_1(self):\n",
    "\n",
    "    def f(x):\n",
    "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 30 * x + 71\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3],[5],[7]])\n",
    "\n",
    "  def poly_2(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x**2 * (x**2 - 16*x + 40)) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return x * (x - 2) * (x - 10)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 24 * x + 20\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[0],[2],[10]])\n",
    "\n",
    "  def poly_3(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x * (3 * x**3 - 64 * x**2 + 414 * x - 648)) / 12\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 1) * (x - 6) * (x - 9)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 32 * x + 69\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[1],[6],[9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EOhfs8acZf6"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>backtracking_alpha() imlements a backtracking linesearch to find a suitable step length as described in the book. steepest_descent() implements the line search steepest descent method.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_b1zjSs3cZf6"
   },
   "outputs": [],
   "source": [
    "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
    "\n",
    "  alpha = alpha0\n",
    "\n",
    "  while not f(xk + alpha * pk) <= (f(xk) + c * alpha * grad_f(xk).T @ pk):\n",
    "    alpha *= rho\n",
    "  \n",
    "  return alpha\n",
    "\n",
    "def steepest_descent(x0, f, grad_f=None):\n",
    "\n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "    conv_tol = 1e-6\n",
    "\n",
    "  i = 0\n",
    "  xk = x0\n",
    "\n",
    "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
    "    \n",
    "    pk = -grad_f(xk)\n",
    "    xk = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
    "    i+=1\n",
    "    \n",
    "  print(f\"\\nsearch terminated at iteration {i}, result: {xk}\")\n",
    "  return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF6N5VLVcZf7"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wXevbYiwcZf7"
   },
   "outputs": [],
   "source": [
    "rseed = [1,4,6,7,8]\n",
    "quadratic_probs = []\n",
    "for i in range(5):\n",
    "  prob = Problem()\n",
    "  prob.quadratic(10, rseed[i])\n",
    "  quadratic_probs.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1TBCRNucZf7"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UfgMEOMLcZf7"
   },
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal,**args)\n",
    "    f_appr = f(x_appr,**args)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(f,x_optimal,args)}\\n')\n",
    "    grad_appr = grad(f,x_appr,args)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSmyQfOLcZf8",
    "outputId": "7708e125-6ada-4fea-edb1-a3db6663b11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 790, result: [-5.00003026  0.99998393  2.00001368 -1.99997113 -1.00000817  1.00003911\n",
      " -5.00000831  4.99995562 -9.99999253  5.99998299]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1067, result: [-5.00000077  0.99999961  2.00000036 -1.99999933 -1.00000026  1.000001\n",
      " -5.0000003   4.99999884 -9.99999983  5.99999957]\n",
      "\n",
      " actual minimum: [ -5   1   2  -2  -1   1  -5   5 -10   6]\n",
      "\n",
      "Problem 2: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 426, result: [ 4.00004218 -5.00001511 -8.99998282 -2.00002312 -2.00001425  7.99998125\n",
      " -0.99998739 -2.99994359  2.99999759 -1.9999755 ]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 572, result: [ 4.00000149 -5.00000065 -8.99999947 -2.00000083 -2.00000047  7.99999938\n",
      " -0.9999996  -2.99999829  2.99999993 -1.99999915]\n",
      "\n",
      " actual minimum: [ 4 -5 -9 -2 -2  8 -1 -3  3 -2]\n",
      "\n",
      "Problem 3: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 875, result: [ 2.58148688e-05 -1.00003058e+00 -6.99999918e+00 -5.55822881e-06\n",
      "  3.00002492e+00  4.99998851e+00  1.57493854e-05  5.99999388e+00\n",
      " -8.99998206e+00  9.99975732e-01]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1107, result: [ 1.49325050e-06 -1.00000186e+00 -6.99999989e+00 -2.83219862e-07\n",
      "  3.00000155e+00  4.99999944e+00  9.13392070e-07  5.99999972e+00\n",
      " -8.99999877e+00  9.99998576e-01]\n",
      "\n",
      " actual minimum: [ 0 -1 -7  0  3  5  0  6 -9  1]\n",
      "\n",
      "Problem 4: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1794, result: [ 5.00003906e+00 -5.99998624e+00 -6.99992783e+00  9.00001875e+00\n",
      " -3.00000282e+00  3.99998554e+00 -1.99993866e+00  3.99997336e+00\n",
      "  2.25870341e-05 -1.99997394e+00]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 2351, result: [ 5.00000155e+00 -5.99999945e+00 -6.99999714e+00  9.00000080e+00\n",
      " -2.99999998e+00  3.99999947e+00 -1.99999753e+00  3.99999902e+00\n",
      "  9.43992727e-07 -1.99999895e+00]\n",
      "\n",
      " actual minimum: [ 5 -6 -7  9 -3  4 -2  4  0 -2]\n",
      "\n",
      "Problem 5: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 414, result: [-6.99999736  6.99999365 -0.99999659 -5.00000307 -1.99999845  8.99999943\n",
      " -2.00000264  5.99999556  2.99999735  7.00000008]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 555, result: [-7.00000002  7.         -1.00000002 -4.99999998 -2.          9.00000001\n",
      " -1.99999999  6.00000003  3.00000002  7.        ]\n",
      "\n",
      " actual minimum: [-7  7 -1 -5 -2  9 -2  6  3  7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, prob in enumerate(quadratic_probs):\n",
    "  print(f\"Problem {i+1}: \")\n",
    "  print(\"approximated gradient: \")\n",
    "  steepest_descent(np.zeros(10), prob.f)\n",
    "  print(\"\\n exact gradient: \")\n",
    "  steepest_descent(np.zeros(10), prob.f, prob.grad_f)\n",
    "  print(f\"\\n actual minimum: {prob.min_x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5rkOOgOcZf8"
   },
   "source": [
    "If there is no gradient providet, the steepest descent algorithm is the slowest of the four.\n",
    "With a gradient provided, it sometimes outperformes the conjugent gradient method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SW5-O0ZcZf8"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN0W_bYycZf9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxJIUhehcZf9"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSEog4wqcZf9",
    "outputId": "dd1350bb-a6c5-444b-c4b7-db36402a70a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem rosenbrock: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 5001, result: [1.00225053 1.00455368]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 5001, result: [1.00225488 1.00455996]\n",
      "\n",
      "actual minimum: [1 1]\n",
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 1834, result: [-3.77931042 -3.2831859 ]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 2816, result: [-3.77931026 -3.28318599]\n",
      "\n",
      "actual minimum: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Problem poly_1: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 178, result: [6.99999526]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 214, result: [7.00000005]\n",
      "\n",
      "actual minimum: [[3]\n",
      " [5]\n",
      " [7]]\n",
      "\n",
      "Problem poly_2: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 5001, result: [9.99999972]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 2591, result: [10.]\n",
      "\n",
      "actual minimum: [[ 0]\n",
      " [ 2]\n",
      " [10]]\n",
      "\n",
      "Problem poly_3: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 257, result: [0.99999981]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 432, result: [1.]\n",
      "\n",
      "actual minimum: [[1]\n",
      " [6]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"Problem rosenbrock: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2,1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([0,0]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_1()\n",
    "print(f\"\\nProblem poly_1: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_2()\n",
    "print(f\"\\nProblem poly_2: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_3()\n",
    "print(f\"\\nProblem poly_3: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iFNIPpjxcZf9"
   },
   "source": [
    "With non quadratic functions the steepest descent algorithm is the slowest of the four almost all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oU43ESvcZf9"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with approximate gradient:\n",
      "\n",
      "search terminated at iteration 169, result: [-9.17534510e-07 -9.17534510e-07 -9.17534510e-07 -9.17534510e-07\n",
      " -9.17534510e-07 -9.17534510e-07 -9.17534510e-07 -9.17534510e-07\n",
      " -9.17534372e-07 -9.17534523e-07]\n",
      "\n",
      "\n",
      "\n",
      "Test with exact gradient:\n",
      "\n",
      "search terminated at iteration 175, result: [-9.82741173e-09 -9.82741173e-09 -9.82741173e-09 -9.82741173e-09\n",
      " -9.82741173e-09 -9.82741173e-09 -9.82741173e-09 -9.82741173e-09\n",
      " -9.82741173e-09 -9.82741173e-09]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algorithm_to_test = steepest_descent\n",
    "\n",
    "# Here you can set your individual starting point\n",
    "x_0 = np.ones(10)\n",
    "\n",
    "# Here you can enter your individual function\n",
    "def f(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "# Here you can enter the exact gradient of your function\n",
    "# This function will just be used in the second test\n",
    "def grad_f(x):\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Test run:\n",
    "\n",
    "print(\"Test with approximate gradient:\")\n",
    "algorithm_to_test(x_0, f)\n",
    "\n",
    "print('\\n'*2) # Print some lines between the tests\n",
    "\n",
    "print(\"Test with exact gradient:\")\n",
    "algorithm_to_test(x_0, f, grad_f)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "phase2_steepest_descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
