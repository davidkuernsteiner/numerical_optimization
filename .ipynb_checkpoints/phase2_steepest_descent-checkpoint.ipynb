{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidkuernsteiner/numerical_optimization/blob/main/phase2_steepest_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9NOe0yxcZfu"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTWMMYGucZfx"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZrrkIhScZfy"
   },
   "source": [
    "<i>Group 1<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">David</td>\n",
    "    <td style = \"text-align: left\">KÃ¼rnsteiner</td>\n",
    "    <td style = \"text-align: left\">11820336</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Elias</td>\n",
    "    <td style = \"text-align: left\">Ramoser</td>\n",
    "    <td style = \"text-align: left\">11918558</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Georg</td>\n",
    "    <td style = \"text-align: left\">Storz</td>\n",
    "    <td style = \"text-align: left\">11918811</td>\n",
    "    </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egf-RyNlcZfz"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<i>All points x are represented as numpy arrays. Function f returns a scalar with grad_f and hessian_f returning numpy arrays.<i>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5rKWVctcZf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *\n",
    "from sklearn.datasets import make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opT-uwtJcZf0"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Function returns True if the gradient of f at xk relative to x0 is smaller than parameter tol. Additionally there is an upper bound for iterations to stop non converging algorithms.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGU8MquEcZf1"
   },
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
    "  if i > max_iter: \n",
    "    return True\n",
    "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBEAwLHcZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90K3ffG0dkFl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Qy9Ci0cZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zsxIWEAcZf3"
   },
   "outputs": [],
   "source": [
    "#your function for stabilising goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpA1FRYDcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTrMirmcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>linear_solve() provides a way to solve linear systems of equations using a LU-factorization of A and subsequent forward and backward substitution as described in the book. This solver proves to be quite unstable though in practical applications. We therefore use the numpy implementation of solve(). <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWSeLfhAcZf4"
   },
   "outputs": [],
   "source": [
    "def forward_substitution(L, b):\n",
    "\n",
    "    n = L.shape[0]\n",
    "\n",
    "    y = np.zeros_like(b, dtype=np.double);\n",
    "\n",
    "    y[0] = b[0] / L[0, 0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        y[i] = (b[i] - np.dot(L[i,:i], y[:i])) / L[i,i]\n",
    "        \n",
    "    return y\n",
    "\n",
    "def back_substitution(U, y):\n",
    "    \n",
    "    n = U.shape[0]\n",
    "\n",
    "    x = np.zeros_like(y, dtype=np.double);\n",
    "\n",
    "    x[-1] = y[-1] / U[-1, -1]\n",
    "    \n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = (y[i] - np.dot(U[i,i:], x[i:])) / U[i,i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def linear_solve(A, b):\n",
    "    \n",
    "    P, L, U = scipy.linalg.lu(A)\n",
    "    \n",
    "    y = forward_substitution(L, P @ b)\n",
    "    \n",
    "    return back_substitution(U, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWsNJuicZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Following functions are wrapper functions that provide approximations of the gradient and hessian of f using the forward-difference approach as described in the book. <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tJcCQY8cZf5"
   },
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "  arr = np.zeros(size)\n",
    "  arr[index] = 1.0\n",
    "  return arr\n",
    "\n",
    "def approx_grad(f, e=1.1*10**-8):\n",
    "  def grad_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + e) - f(x)) / e\n",
    "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "  return grad_f\n",
    "\n",
    "def approx_hessian(f, e=1.1*10**-8):\n",
    "  def hessian_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
    "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
    "  return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkKtvzncZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>The class Problem() provides an object to generate and set up quadratic and non some non quadratic test problems for the algorithms with.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqyBMX7DcZf6"
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    self.f = None\n",
    "    self.grad_f = None\n",
    "    self.hessian_f = None\n",
    "    self.min_x = None\n",
    "\n",
    "  def quadratic(self, n_dim, rseed):\n",
    "\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    A = make_spd_matrix(n_dim, random_state=rseed)\n",
    "    x = rng.randint(-10, 10, n_dim)\n",
    "    b = A @ x\n",
    "\n",
    "    def f(x):\n",
    "      return 0.5 * x.T @ A @ x - b @ x\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return A @ x - b\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return A\n",
    "    \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = x\n",
    "    self.A = A\n",
    "    self.b = b\n",
    "\n",
    "  def rosenbrock(self):\n",
    "\n",
    "    def f(x):\n",
    "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(x):\n",
    "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "           200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
    "           [-400*x[0], 200]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([1,1])\n",
    "\n",
    "  def himmelblau(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
    "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
    "\n",
    "  def poly_1(self):\n",
    "\n",
    "    def f(x):\n",
    "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 30 * x + 71\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3],[5],[7]])\n",
    "\n",
    "  def poly_2(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x**2 * (x**2 - 16*x + 40)) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return x * (x - 2) * (x - 10)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 24 * x + 20\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[0],[2],[10]])\n",
    "\n",
    "  def poly_3(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x * (3 * x**3 - 64 * x**2 + 414 * x - 648)) / 12\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 1) * (x - 6) * (x - 9)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 32 * x + 69\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[1],[6],[9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EOhfs8acZf6"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>backtracking_alpha() imlements a backtracking linesearch to find a suitable step length as described in the book. steepest_descent() implements the line search steepest descent method.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b1zjSs3cZf6"
   },
   "outputs": [],
   "source": [
    "def backtracking_alpha(f, grad_f, xk, pk, alpha0=1, rho=0.95, c=1e-4):\n",
    "\n",
    "  alpha = alpha0\n",
    "\n",
    "  while not f(xk + alpha * pk) <= (f(xk) + c * alpha * grad_f(xk).T @ pk):\n",
    "    alpha *= rho\n",
    "  \n",
    "  return alpha\n",
    "\n",
    "def steepest_descent(x0, f, grad_f=None):\n",
    "\n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "    conv_tol = 1e-6\n",
    "\n",
    "  i = 0\n",
    "  xk = x0\n",
    "\n",
    "  while not stop_crit(grad_f, xk, x0, i, tol=conv_tol):\n",
    "    \n",
    "    pk = -grad_f(xk)\n",
    "    xk = xk + backtracking_alpha(f, grad_f, xk, pk) * pk\n",
    "    i+=1\n",
    "    \n",
    "  print(f\"\\nsearch terminated at iteration {i}, result: {xk}\")\n",
    "  return xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF6N5VLVcZf7"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXevbYiwcZf7"
   },
   "outputs": [],
   "source": [
    "rseed = [1,4,6,7,8]\n",
    "quadratic_probs = []\n",
    "for i in range(5):\n",
    "  prob = Problem()\n",
    "  prob.quadratic(10, rseed[i])\n",
    "  quadratic_probs.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1TBCRNucZf7"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfgMEOMLcZf7"
   },
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal,**args)\n",
    "    f_appr = f(x_appr,**args)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(f,x_optimal,args)}\\n')\n",
    "    grad_appr = grad(f,x_appr,args)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSmyQfOLcZf8",
    "outputId": "7708e125-6ada-4fea-edb1-a3db6663b11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 780, result: [-5.00003293  0.99998283  2.00001434 -1.99996867 -1.00000987  1.00004505\n",
      " -5.00000942  4.99995231 -9.99999204  5.99998151]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1104, result: [-5.00000049  0.99999976  2.00000023 -1.99999958 -1.00000017  1.00000063\n",
      " -5.0000002   4.99999926 -9.99999989  5.99999973]\n",
      "\n",
      " actual minimum: [ -5   1   2  -2  -1   1  -5   5 -10   6]\n",
      "\n",
      "Problem 2: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 439, result: [ 4.00004135 -5.00001562 -8.99999011 -2.00002033 -2.00000893  7.99998124\n",
      " -0.99999261 -2.99996226  3.00000037 -1.99998334]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 581, result: [ 4.00000124 -5.00000053 -8.9999996  -2.00000068 -2.00000039  7.99999948\n",
      " -0.99999969 -2.99999864  2.99999996 -1.99999931]\n",
      "\n",
      " actual minimum: [ 4 -5 -9 -2 -2  8 -1 -3  3 -2]\n",
      "\n",
      "Problem 3: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 892, result: [ 2.15836830e-05 -1.00002583e+00 -6.99999614e+00 -4.28057442e-06\n",
      "  3.00001988e+00  4.99999410e+00  1.25811700e-05  5.99999533e+00\n",
      " -8.99998216e+00  9.99981171e-01]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1144, result: [ 9.31592348e-07 -1.00000117e+00 -6.99999992e+00 -1.75526162e-07\n",
      "  3.00000097e+00  4.99999966e+00  5.60903404e-07  5.99999982e+00\n",
      " -8.99999923e+00  9.99999119e-01]\n",
      "\n",
      " actual minimum: [ 0 -1 -7  0  3  5  0  6 -9  1]\n",
      "\n",
      "Problem 4: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1823, result: [ 5.00003724e+00 -5.99998724e+00 -6.99993049e+00  9.00001983e+00\n",
      " -2.99999712e+00  3.99998806e+00 -1.99993900e+00  3.99997813e+00\n",
      "  2.41263992e-05 -1.99997531e+00]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 2342, result: [ 5.00000170e+00 -5.99999940e+00 -6.99999687e+00  9.00000088e+00\n",
      " -2.99999998e+00  3.99999941e+00 -1.99999729e+00  3.99999892e+00\n",
      "  1.03711734e-06 -1.99999885e+00]\n",
      "\n",
      " actual minimum: [ 5 -6 -7  9 -3  4 -2  4  0 -2]\n",
      "\n",
      "Problem 5: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 430, result: [-6.99999408  7.00000039 -0.99999671 -5.0000025  -1.99999641  9.00000155\n",
      " -2.00000056  5.99999627  2.99999761  7.00000384]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 565, result: [-7.  7. -1. -5. -2.  9. -2.  6.  3.  7.]\n",
      "\n",
      " actual minimum: [-7  7 -1 -5 -2  9 -2  6  3  7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, prob in enumerate(quadratic_probs):\n",
    "  print(f\"Problem {i+1}: \")\n",
    "  print(\"approximated gradient: \")\n",
    "  steepest_descent(np.zeros(10), prob.f)\n",
    "  print(\"\\n exact gradient: \")\n",
    "  steepest_descent(np.zeros(10), prob.f, prob.grad_f)\n",
    "  print(f\"\\n actual minimum: {prob.min_x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5rkOOgOcZf8"
   },
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SW5-O0ZcZf8"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN0W_bYycZf9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxJIUhehcZf9"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSEog4wqcZf9",
    "outputId": "dd1350bb-a6c5-444b-c4b7-db36402a70a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem rosenbrock: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 5001, result: [1.00225053 1.00455368]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 5001, result: [1.00225488 1.00455996]\n",
      "\n",
      "actual minimum: [1 1]\n",
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 1834, result: [-3.77931042 -3.2831859 ]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 2816, result: [-3.77931026 -3.28318599]\n",
      "\n",
      "actual minimum: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Problem poly_1: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 178, result: [6.99999526]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 214, result: [7.00000005]\n",
      "\n",
      "actual minimum: [[3]\n",
      " [5]\n",
      " [7]]\n",
      "\n",
      "Problem poly_2: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 5001, result: [9.99999972]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 2591, result: [10.]\n",
      "\n",
      "actual minimum: [[ 0]\n",
      " [ 2]\n",
      " [10]]\n",
      "\n",
      "Problem poly_3: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 257, result: [0.99999981]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 432, result: [1.]\n",
      "\n",
      "actual minimum: [[1]\n",
      " [6]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"Problem rosenbrock: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2,1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([0,0]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_1()\n",
    "print(f\"\\nProblem poly_1: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_2()\n",
    "print(f\"\\nProblem poly_2: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_3()\n",
    "print(f\"\\nProblem poly_3: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "steepest_descent(np.array([1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFNIPpjxcZf9"
   },
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oU43ESvcZf9"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "phase2_steepest_descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
