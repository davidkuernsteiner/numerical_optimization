{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidkuernsteiner/numerical_optimization/blob/main/phase2_newton_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9NOe0yxcZfu"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTWMMYGucZfx"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZrrkIhScZfy"
   },
   "source": [
    "<i>Group 1<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">David</td>\n",
    "    <td style = \"text-align: left\">KÃ¼rnsteiner</td>\n",
    "    <td style = \"text-align: left\">11820336</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Elias</td>\n",
    "    <td style = \"text-align: left\">Ramoser</td>\n",
    "    <td style = \"text-align: left\">11918558</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Georg</td>\n",
    "    <td style = \"text-align: left\">Storz</td>\n",
    "    <td style = \"text-align: left\">11918811</td>\n",
    "    </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egf-RyNlcZfz"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<i>All points x are represented as numpy arrays. Function f returns a scalar with grad_f and hessian_f returning numpy arrays.<i>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5rKWVctcZf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *\n",
    "from sklearn.datasets import make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opT-uwtJcZf0"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Function returns True if the gradient of f at xk relative to x0 is smaller than parameter tol. Additionally there is an upper bound for iterations to stop non converging algorithms.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGU8MquEcZf1"
   },
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
    "  if i > max_iter: \n",
    "    return True\n",
    "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBEAwLHcZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90K3ffG0dkFl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Qy9Ci0cZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zsxIWEAcZf3"
   },
   "outputs": [],
   "source": [
    "#your function for stabilising goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpA1FRYDcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTrMirmcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>linear_solve() provides a way to solve linear systems of equations using a LU-factorization of A and subsequent forward and backward substitution as described in the book. This solver proves to be quite unstable though in practical applications. We therefore use the numpy implementation of solve(). <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWSeLfhAcZf4"
   },
   "outputs": [],
   "source": [
    "def forward_substitution(L, b):\n",
    "\n",
    "    n = L.shape[0]\n",
    "\n",
    "    y = np.zeros_like(b, dtype=np.double);\n",
    "\n",
    "    y[0] = b[0] / L[0, 0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        y[i] = (b[i] - np.dot(L[i,:i], y[:i])) / L[i,i]\n",
    "        \n",
    "    return y\n",
    "\n",
    "def back_substitution(U, y):\n",
    "    \n",
    "    n = U.shape[0]\n",
    "\n",
    "    x = np.zeros_like(y, dtype=np.double);\n",
    "\n",
    "    x[-1] = y[-1] / U[-1, -1]\n",
    "    \n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = (y[i] - np.dot(U[i,i:], x[i:])) / U[i,i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def linear_solve(A, b):\n",
    "    \n",
    "    P, L, U = scipy.linalg.lu(A)\n",
    "    \n",
    "    y = forward_substitution(L, P @ b)\n",
    "    \n",
    "    return back_substitution(U, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWsNJuicZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Following functions are wrapper functions that provide approximations of the gradient and hessian of f using the forward-difference approach as described in the book. <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tJcCQY8cZf5"
   },
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "  arr = np.zeros(size)\n",
    "  arr[index] = 1.0\n",
    "  return arr\n",
    "\n",
    "def approx_grad(f, e=1.1*10**-8):\n",
    "  def grad_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + e) - f(x)) / e\n",
    "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "  return grad_f\n",
    "\n",
    "def approx_hessian(f, e=1.1*10**-8):\n",
    "  def hessian_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
    "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
    "  return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkKtvzncZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>The class Problem() provides an object to generate and set up quadratic and non some non quadratic test problems for the algorithms with.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqyBMX7DcZf6"
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    self.f = None\n",
    "    self.grad_f = None\n",
    "    self.hessian_f = None\n",
    "    self.min_x = None\n",
    "\n",
    "  def quadratic(self, n_dim, rseed):\n",
    "\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    A = make_spd_matrix(n_dim, random_state=rseed)\n",
    "    x = rng.randint(-10, 10, n_dim)\n",
    "    b = A @ x\n",
    "\n",
    "    def f(x):\n",
    "      return 0.5 * x.T @ A @ x - b @ x\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return A @ x - b\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return A\n",
    "    \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = x\n",
    "    self.A = A\n",
    "    self.b = b\n",
    "\n",
    "  def rosenbrock(self):\n",
    "\n",
    "    def f(x):\n",
    "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(x):\n",
    "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "           200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
    "           [-400*x[0], 200]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([1,1])\n",
    "\n",
    "  def himmelblau(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
    "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
    "\n",
    "  def poly_1(self):\n",
    "\n",
    "    def f(x):\n",
    "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 30 * x + 71\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3],[5],[7]])\n",
    "\n",
    "  def poly_2(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x**2 * (x**2 - 16*x + 40)) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return x * (x - 2) * (x - 10)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 24 * x + 20\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[0],[2],[10]])\n",
    "\n",
    "  def poly_3(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x * (3 * x**3 - 64 * x**2 + 414 * x - 648)) / 12\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 1) * (x - 6) * (x - 9)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 32 * x + 69\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[1],[6],[9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EOhfs8acZf6"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in [1]. newton_method() implements the line search Newton method solving the equation H @ pk = -grad instead of computing the inverse of H. This is done using np.solve() for stability reasons.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b1zjSs3cZf6"
   },
   "outputs": [],
   "source": [
    "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
    "\n",
    "  alpha = 0\n",
    "  beta = np.Inf\n",
    "  t = 1\n",
    "\n",
    "  while True:\n",
    "\n",
    "    if f(xk + t * pk) > (f(xk) + c1 * t * (pk @ grad_f(xk))):\n",
    "      beta = t\n",
    "      t = 0.5 * (alpha + beta)\n",
    "    elif (-pk @ grad_f(xk + t * pk)) > (-c2 * pk @ grad_f(xk)):\n",
    "      alpha = t\n",
    "      t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
    "    else:\n",
    "      return t\n",
    "\n",
    "def newton_method(x0, f, grad_f=None, hessian_f=None):\n",
    "  \n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "    hessian_f = approx_hessian(f)\n",
    "    conv_tol = 1e-6\n",
    "  i = 0\n",
    "  x = x0\n",
    "  \n",
    "  while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
    "    \n",
    "    #pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else - inv(hessian_f(x)) @ grad_f(x)\n",
    "    pk = np.array([- (1 / hessian_f(x)) @ grad_f(x)]) if x.size == 1 else solve(hessian_f(x), -grad_f(x))\n",
    "    x = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "    i += 1\n",
    "  \n",
    "  print(f\"\\nsearch terminated at iteration {i}, result: {x}\")\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF6N5VLVcZf7"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXevbYiwcZf7"
   },
   "outputs": [],
   "source": [
    "rseed = [1,4,6,7,8]\n",
    "quadratic_probs = []\n",
    "for i in range(5):\n",
    "  prob = Problem()\n",
    "  prob.quadratic(10, rseed[i])\n",
    "  quadratic_probs.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1TBCRNucZf7"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfgMEOMLcZf7"
   },
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal,**args)\n",
    "    f_appr = f(x_appr,**args)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(f,x_optimal,args)}\\n')\n",
    "    grad_appr = grad(f,x_appr,args)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSmyQfOLcZf8",
    "outputId": "34e2870a-832d-49a4-d486-f4f25adcb4ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ -4.99997955   1.00000957   1.99998539  -2.00002401  -0.99998888\n",
      "   0.99996616  -4.99998887   5.0000258  -10.00000857   6.00001461]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ -5.   1.   2.  -2.  -1.   1.  -5.   5. -10.   6.]\n",
      "\n",
      " actual minimum: [ -5   1   2  -2  -1   1  -5   5 -10   6]\n",
      "\n",
      "Problem 2: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ 4.00002641 -5.00000214 -8.99998134 -1.99999049 -1.99999525  7.99999422\n",
      " -1.00000527 -2.9999853   3.00000018 -1.99998029]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ 4. -5. -9. -2. -2.  8. -1. -3.  3. -2.]\n",
      "\n",
      " actual minimum: [ 4 -5 -9 -2 -2  8 -1 -3  3 -2]\n",
      "\n",
      "Problem 3: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ 1.36831717e-05 -1.00001150e+00 -6.99999952e+00 -5.37699129e-06\n",
      "  3.00000750e+00  4.99999845e+00  5.10036059e-06  5.99999768e+00\n",
      " -8.99998538e+00  9.99983468e-01]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ 5.06726707e-15 -1.00000000e+00 -7.00000000e+00 -1.69048865e-15\n",
      "  3.00000000e+00  5.00000000e+00  5.70078536e-15  6.00000000e+00\n",
      " -9.00000000e+00  1.00000000e+00]\n",
      "\n",
      " actual minimum: [ 0 -1 -7  0  3  5  0  6 -9  1]\n",
      "\n",
      "Problem 4: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ 5.00000286e+00 -5.99999704e+00 -7.00000180e+00  8.99999899e+00\n",
      " -3.00000050e+00  4.00000182e+00 -1.99999971e+00  4.00000270e+00\n",
      "  1.05240020e-06 -1.99999963e+00]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [ 5.00000000e+00 -6.00000000e+00 -7.00000000e+00  9.00000000e+00\n",
      " -3.00000000e+00  4.00000000e+00 -2.00000000e+00  4.00000000e+00\n",
      " -1.04612504e-14 -2.00000000e+00]\n",
      "\n",
      " actual minimum: [ 5 -6 -7  9 -3  4 -2  4  0 -2]\n",
      "\n",
      "Problem 5: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [-6.99999355  6.99998212 -1.00001181 -4.99999772 -2.00000667  9.00001033\n",
      " -2.00001187  6.00000301  2.99998436  6.99999919]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 1, result: [-7.  7. -1. -5. -2.  9. -2.  6.  3.  7.]\n",
      "\n",
      " actual minimum: [-7  7 -1 -5 -2  9 -2  6  3  7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, prob in enumerate(quadratic_probs):\n",
    "  print(f\"Problem {i+1}: \")\n",
    "  print(\"approximated gradient: \")\n",
    "  newton_method(np.zeros(10), prob.f)\n",
    "  print(\"\\n exact gradient: \")\n",
    "  newton_method(np.zeros(10), prob.f, prob.grad_f, prob.hessian_f)\n",
    "  print(f\"\\n actual minimum: {prob.min_x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5rkOOgOcZf8"
   },
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SW5-O0ZcZf8"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN0W_bYycZf9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxJIUhehcZf9"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSEog4wqcZf9",
    "outputId": "619a70d7-f688-4285-c8a8-045df3a568dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem rosenbrock: \n",
      "\n",
      "approximate gradient/hessian: \n",
      "\n",
      "search terminated at iteration 6, result: [0.99999631 0.99999261]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 8, result: [1. 1.]\n",
      "\n",
      "actual minimum: [1 1]\n",
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "approximate gradient/hessian: \n",
      "\n",
      "search terminated at iteration 5001, result: [2.5 1.5]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 5, result: [3. 2.]\n",
      "\n",
      "actual minimum: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Problem poly_1: \n",
      "\n",
      "approximate gradient/hessian: \n",
      "\n",
      "search terminated at iteration 6, result: [2.99999982]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 6, result: [3.]\n",
      "\n",
      "actual minimum: [[3]\n",
      " [5]\n",
      " [7]]\n",
      "\n",
      "Problem poly_2: \n",
      "\n",
      "approximate gradient/hessian: \n",
      "\n",
      "search terminated at iteration 5001, result: [9.57583333]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 6, result: [10.]\n",
      "\n",
      "actual minimum: [[ 0]\n",
      " [ 2]\n",
      " [10]]\n",
      "\n",
      "Problem poly_3: \n",
      "\n",
      "approximate gradient/hessian: \n",
      "\n",
      "search terminated at iteration 5001, result: [8.69881056]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 4, result: [9.]\n",
      "\n",
      "actual minimum: [[1]\n",
      " [6]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"Problem rosenbrock: \\n\")\n",
    "print(\"approximate gradient/hessian: \")\n",
    "newton_method(np.array([1.2,1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "newton_method(np.array([1.2,1.2]), prob.f, prob.grad_f, prob.hessian_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"approximate gradient/hessian: \")\n",
    "newton_method(np.array([2.5,1.5]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "newton_method(np.array([2.5,1.75]), prob.f, prob.grad_f, prob.hessian_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_1()\n",
    "print(f\"\\nProblem poly_1: \\n\")\n",
    "print(\"approximate gradient/hessian: \")\n",
    "newton_method(np.array([1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "newton_method(np.array([1.2]), prob.f, prob.grad_f, prob.hessian_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_2()\n",
    "print(f\"\\nProblem poly_2: \\n\")\n",
    "print(\"approximate gradient/hessian: \")\n",
    "newton_method(np.array([9.5]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "newton_method(np.array([1.2]), prob.f, prob.grad_f, prob.hessian_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_3()\n",
    "print(f\"\\nProblem poly_3: \\n\")\n",
    "print(\"approximate gradient/hessian: \")\n",
    "newton_method(np.array([8.5]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "newton_method(np.array([8.5]), prob.f, prob.grad_f, prob.hessian_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFNIPpjxcZf9"
   },
   "source": [
    "<i>Here is some place for your analysis. How the behavour of algorithm changed after adjustments? What are specific details, differences you noticed with respect to other algorithms behaviour.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oU43ESvcZf9"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "595FL4QTV3Gc"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "phase2_newton_method.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
