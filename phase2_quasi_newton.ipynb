{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidkuernsteiner/numerical_optimization/blob/main/phase2_quasi_newton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9NOe0yxcZfu"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Numerical Optimisation. Project 1<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTWMMYGucZfx"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Team Information<br></h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZrrkIhScZfy"
   },
   "source": [
    "<i>Group 1<br>\n",
    "Participants information in alphabetical order</i>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th style = \"text-align: left\">#</th>\n",
    "    <th style = \"text-align: left\">Name</th>\n",
    "    <th style = \"text-align: left\">Lastname</th>\n",
    "    <th style = \"text-align: left\">Matr Number</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">1</td>\n",
    "    <td style = \"text-align: left\">David</td>\n",
    "    <td style = \"text-align: left\">KÃ¼rnsteiner</td>\n",
    "    <td style = \"text-align: left\">11820336</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">2</td>\n",
    "    <td style = \"text-align: left\">Christian</td>\n",
    "    <td style = \"text-align: left\">Peinthor</td>\n",
    "    <td style = \"text-align: left\">11815592</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">3</td>\n",
    "    <td style = \"text-align: left\">Elias</td>\n",
    "    <td style = \"text-align: left\">Ramoser</td>\n",
    "    <td style = \"text-align: left\">11918558</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style = \"text-align: left\">4</td>\n",
    "    <td style = \"text-align: left\">Georg</td>\n",
    "    <td style = \"text-align: left\">Storz</td>\n",
    "    <td style = \"text-align: left\">11918811</td>\n",
    "    </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egf-RyNlcZfz"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Implementation<br></h3><br>\n",
    "<i>All points x are represented as numpy arrays. Function f returns a scalar with grad_f and hessian_f returning numpy arrays.<i>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Imports<br></h4><br>\n",
    "<i>Describe how to install additional packages, if you have some, here</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H5rKWVctcZf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.linalg import *\n",
    "from sklearn.datasets import make_spd_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opT-uwtJcZf0"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stopping criteria<br></h4><br>\n",
    "<i>Function returns True if the gradient of f at xk relative to x0 is smaller than parameter tol. Additionally there is an upper bound for iterations to stop non converging algorithms.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZGU8MquEcZf1"
   },
   "outputs": [],
   "source": [
    "def stop_crit(grad_f, xk, x0, i, tol=1e-8, max_iter=5000):\n",
    "  if i > max_iter: \n",
    "    return True\n",
    "  elif norm(grad_f(xk)) <= tol * norm(grad_f(x0)):\n",
    "    return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBEAwLHcZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Varibales scaling<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90K3ffG0dkFl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2Qy9Ci0cZf2"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Stabilising algorithm<br></h4><br>\n",
    "<i>Place your reasoning here, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9zsxIWEAcZf3"
   },
   "outputs": [],
   "source": [
    "#your function for stabilising goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpA1FRYDcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Fighting floating-point numbers and roundoff error<br></h4><br>\n",
    "<i>Place your reasoning, how your algorithm behave with respect to this problem. You can also try rescaling your problems\n",
    "This is additional task, which can earn you several points.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTrMirmcZf4"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Inverting matrices<br></h4><br>\n",
    "<i>linear_solve() provides a way to solve linear systems of equations using a LU-factorization of A and subsequent forward and backward substitution as described in the book. This solver proves to be quite unstable though in practical applications. We therefore use the numpy implementation of solve(). <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wWSeLfhAcZf4"
   },
   "outputs": [],
   "source": [
    "def forward_substitution(L, b):\n",
    "\n",
    "    n = L.shape[0]\n",
    "\n",
    "    y = np.zeros_like(b, dtype=np.double);\n",
    "\n",
    "    y[0] = b[0] / L[0, 0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        y[i] = (b[i] - np.dot(L[i,:i], y[:i])) / L[i,i]\n",
    "        \n",
    "    return y\n",
    "\n",
    "def back_substitution(U, y):\n",
    "    \n",
    "    n = U.shape[0]\n",
    "\n",
    "    x = np.zeros_like(y, dtype=np.double);\n",
    "\n",
    "    x[-1] = y[-1] / U[-1, -1]\n",
    "    \n",
    "    for i in range(n-2, -1, -1):\n",
    "        x[i] = (y[i] - np.dot(U[i,i:], x[i:])) / U[i,i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "def linear_solve(A, b):\n",
    "    \n",
    "    P, L, U = scipy.linalg.lu(A)\n",
    "    \n",
    "    y = forward_substitution(L, P @ b)\n",
    "    \n",
    "    return back_substitution(U, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnWsNJuicZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Gradients calculation<br></h4><br>\n",
    "<i>Following functions are wrapper functions that provide approximations of the gradient and hessian of f using the forward-difference approach as described in the book. <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1tJcCQY8cZf5"
   },
   "outputs": [],
   "source": [
    "def e_i(size, index):\n",
    "  arr = np.zeros(size)\n",
    "  arr[index] = 1.0\n",
    "  return arr\n",
    "\n",
    "def approx_grad(f, e=1.1*10**-8):\n",
    "  def grad_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + e) - f(x)) / e\n",
    "    return np.array([(f(x + e * e_i(x.size, i)) - f(x)) / e for i in range(x.size)])\n",
    "  return grad_f\n",
    "\n",
    "def approx_hessian(f, e=1.1*10**-8):\n",
    "  def hessian_f(x):\n",
    "    if x.size == 1:\n",
    "      return (f(x + 2*e) - 2*f(x + e) + f(x)) / e**2\n",
    "    return np.array([[(f(x + e * e_i(x.size, i) + e * e_i(x.size, j)) - f(\n",
    "                      x + e * e_i(x.size, i)) - f(x + e * e_i(x.size, j)) + f(\n",
    "                      x)) / e**2 for j in range(x.size)] for i in range(x.size)])\n",
    "  return hessian_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWkKtvzncZf5"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Additional objects you implemented<br></h4><br>\n",
    "<i>The class Problem() provides an object to generate and set up quadratic and non some non quadratic test problems for the algorithms with.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lqyBMX7DcZf6"
   },
   "outputs": [],
   "source": [
    "class Problem():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    self.f = None\n",
    "    self.grad_f = None\n",
    "    self.hessian_f = None\n",
    "    self.min_x = None\n",
    "\n",
    "  def quadratic(self, n_dim, rseed):\n",
    "\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    A = make_spd_matrix(n_dim, random_state=rseed)\n",
    "    x = rng.randint(-10, 10, n_dim)\n",
    "    b = A @ x\n",
    "\n",
    "    def f(x):\n",
    "      return 0.5 * x.T @ A @ x - b @ x\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return A @ x - b\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return A\n",
    "    \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = x\n",
    "    self.A = A\n",
    "    self.b = b\n",
    "\n",
    "  def rosenbrock(self):\n",
    "\n",
    "    def f(x):\n",
    "      return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "    def grad_f(x):\n",
    "      return np.array([-400*x[0]*(x[1] - x[0]**2) - 2*(1 - x[0]),\n",
    "           200*(x[1] - x[0]**2)])\n",
    "\n",
    "    def hessian_f(x):\n",
    "      return np.array([[-400*(x[1] - 3*x[0]**2) + 2, -400*x[0]],\n",
    "           [-400*x[0], 200]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([1,1])\n",
    "\n",
    "  def himmelblau(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return np.array([4*x[0]*(x[0]**2 + x[1] - 11)+2*(x[0] + x[1]**2 - 7),\n",
    "                       4*x[1]*(x[1]**2 + x[0] - 7)+2*(x[1] + x[0]**2 - 11)])\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return np.array([[12*x[0]**2 + 4*x[1] - 42, 4*(x[1] + x[0])],\n",
    "                       [4*(x[1] + x[0]), 12*x[1]**2 + 4*x[0] - 26]])\n",
    "      \n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3,2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.584428, -1.848126]])\n",
    "\n",
    "  def poly_1(self):\n",
    "\n",
    "    def f(x):\n",
    "      return ((x - 7)**2 * (x - 3)**2) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 7) * (x - 5) * (x - 3)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 30 * x + 71\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[3],[5],[7]])\n",
    "\n",
    "  def poly_2(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x**2 * (x**2 - 16*x + 40)) / 4\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return x * (x - 2) * (x - 10)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 24 * x + 20\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[0],[2],[10]])\n",
    "\n",
    "  def poly_3(self):\n",
    "\n",
    "    def f(x):\n",
    "      return (x * (3 * x**3 - 64 * x**2 + 414 * x - 648)) / 12\n",
    "    \n",
    "    def grad_f(x):\n",
    "      return (x - 1) * (x - 6) * (x - 9)\n",
    "    \n",
    "    def hessian_f(x):\n",
    "      return 3 * x**2 - 32 * x + 69\n",
    "\n",
    "    self.f = f\n",
    "    self.grad_f = grad_f\n",
    "    self.hessian_f = hessian_f\n",
    "    self.min_x = np.array([[1],[6],[9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EOhfs8acZf6"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Optimising algorithm itself<br></h4><br>\n",
    "<i>alpha_wolfe() returns a step length satisfiying the weak wolfe conditions using a bisection approach as described in [1]. \n",
    "SR1() implements the line search quasi newton method utilizing SR1 updating for inverse Hessian approximation and a method for stabilizing that resets H as a multiple of I inspired by the method deployed in [2].<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_b1zjSs3cZf6"
   },
   "outputs": [],
   "source": [
    "def alpha_wolfe(f, grad_f, xk, pk, c1=1e-4, c2=0.9):\n",
    "\n",
    "  alpha = 0\n",
    "  beta = np.Inf\n",
    "  t = 1\n",
    "\n",
    "  while True:\n",
    "\n",
    "    if f(xk + t * pk) > (f(xk) + c1 * t * (pk @ grad_f(xk))):\n",
    "      beta = t\n",
    "      t = 0.5 * (alpha + beta)\n",
    "    elif (-pk @ grad_f(xk + t * pk)) > (-c2 * pk @ grad_f(xk)):\n",
    "      alpha = t\n",
    "      t = (2 * alpha if beta == np.Inf else 0.5 * (alpha + beta))\n",
    "    else:\n",
    "      return t\n",
    "\n",
    "def SR1(x0 : np.array, f, grad_f=None, r=1e-8):\n",
    "\n",
    "  conv_tol = 1e-8\n",
    "  if grad_f == None:\n",
    "    grad_f = approx_grad(f)\n",
    "    conv_tol = 1e-6\n",
    "\n",
    "  i = 0\n",
    "  H = np.identity(x0.size)\n",
    "  x = x0\n",
    "\n",
    "  while not stop_crit(grad_f, x, x0, i, tol=conv_tol):\n",
    "  \n",
    "    pk = - H @ grad_f(x)\n",
    "    x_1 = x + alpha_wolfe(f, grad_f, x, pk) * pk\n",
    "    sk = x_1 - x\n",
    "    yk = grad_f(x_1) - grad_f(x)\n",
    "    rhok = 1 / (yk.T @ sk)\n",
    "    \n",
    "    if ((sk @ yk - yk @ H @ yk) < 0) or (abs(yk @ (sk - H @ yk)) < r * np.linalg.norm(yk) * np.linalg.norm(sk - H @ yk)) or (\n",
    "        norm(H, np.inf) > 1e10):\n",
    "      mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n",
    "      H_1 = mu * np.identity(x0.size)\n",
    "      H = H_1\n",
    "    else:\n",
    "      H_1 = H + np.outer((sk - H @ yk), (sk - H @ yk)) / ((sk - H @ yk) @ yk)\n",
    "      H = H_1\n",
    "    x = x_1\n",
    "    i += 1\n",
    "\n",
    "  print(f\"\\nsearch terminated at iteration {i} | result: {x}\")\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF6N5VLVcZf7"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on 5-10 variables, Quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wXevbYiwcZf7"
   },
   "outputs": [],
   "source": [
    "rseed = [1,4,6,7,8]\n",
    "quadratic_probs = []\n",
    "for i in range(5):\n",
    "  prob = Problem()\n",
    "  prob.quadratic(10, rseed[i])\n",
    "  quadratic_probs.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1TBCRNucZf7"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><b>Note:</b> After every test print out the resulsts. \n",
    "<br>For your convinience we implemented a function which will do it for you. Function can be used in case after running optimisation you return $x_{optimal}$, and if you have implemented your gradient approximation. Feel free to bring your adjustments.\n",
    "<br> Additionaly print how many iterations your algotithm needed. You might also provide charts of your taste (if you want).\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UfgMEOMLcZf7"
   },
   "outputs": [],
   "source": [
    "def final_printout(x_0,x_optimal,x_appr,f,grad,args,tolerance):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    x_0: numpy 1D array, corresponds to initial point\n",
    "    x_optimal: numpy 1D array, corresponds to optimal point, which you know, or have solved analytically\n",
    "    x_appr: numpy 1D array, corresponds to approximated point, which your algorithm returned\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    f: function which takes 2 inputs: x (initial, optimal, or approximated)\n",
    "                                      **args\n",
    "       Function f returns a scalar output.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    grad: function which takes 3 inputs: x (initial, optimal, or approximated), \n",
    "                                         function f,\n",
    "                                         args (which are submitted, because you might need\n",
    "                                              to call f(x,**args) inside your gradient function implementation). \n",
    "          Function grad approximates gradient at given point and returns a 1d np array.\n",
    "    --------------------------------------------------------------------------------------------------------------\n",
    "    args: dictionary, additional (except of x) arguments to function f\n",
    "    tolerance: float number, absolute tolerance, precision to which, you compare optimal and approximated solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Initial x is :\\t\\t{x_0}')\n",
    "    print(f'Optimal x is :\\t\\t{x_optimal}')\n",
    "    print(f'Approximated x is :\\t{x_appr}')\n",
    "    print(f'Is close verificaion: \\t{np.isclose(x_appr,x_optimal,atol=tolerance)}\\n')\n",
    "    f_opt = f(x_optimal,**args)\n",
    "    f_appr = f(x_appr,**args)\n",
    "    print(f'Function value in optimal point:\\t{f_opt}')\n",
    "    print(f'Function value in approximated point:   {f_appr}')\n",
    "    print(f'Is close verificaion:\\t{np.isclose(f_opt,f_appr,atol=tolerance)}\\n')\n",
    "    print(f'Gradient approximation in optimal point is:\\n{grad(f,x_optimal,args)}\\n')\n",
    "    grad_appr = grad(f,x_appr,args)\n",
    "    print(f'Gradient approximation in approximated point is:\\n{grad_appr}\\n')\n",
    "    print(f'Is close verificaion:\\n{np.isclose(grad_appr,np.zeros(grad_appr.shape),atol=tolerance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSmyQfOLcZf8",
    "outputId": "2fc6d7ea-e363-4441-cac3-4fbb18b99bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 58 | result: [-5.0001946   0.99989553  2.00008187 -1.99983473 -1.00006189  1.00024194\n",
      " -5.00007264  4.99971227 -9.99995911  5.99988959]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 25 | result: [ -4.9999994    1.00000014   1.99999957  -2.00000079  -0.99999977\n",
      "   0.99999874  -4.99999975   5.00000096 -10.00000026   6.00000034]\n",
      "\n",
      " actual minimum: [ -5   1   2  -2  -1   1  -5   5 -10   6]\n",
      "\n",
      "Problem 2: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 26 | result: [ 4.00024531 -5.00008354 -8.99991686 -2.00011483 -2.00004278  7.99995868\n",
      " -0.99993076 -2.99977594  3.00002912 -1.9998709 ]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 14 | result: [ 4. -5. -9. -2. -2.  8. -1. -3.  3. -2.]\n",
      "\n",
      " actual minimum: [ 4 -5 -9 -2 -2  8 -1 -3  3 -2]\n",
      "\n",
      "Problem 3: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 63 | result: [ 8.88034994e-05 -1.00010929e+00 -6.99998888e+00 -4.56988777e-06\n",
      "  3.00010501e+00  4.99996078e+00  5.46075263e-05  5.99998395e+00\n",
      " -8.99992978e+00  9.99913716e-01]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 14 | result: [-8.29467571e-13 -1.00000000e+00 -7.00000000e+00 -4.20259313e-13\n",
      "  3.00000000e+00  5.00000000e+00 -1.26430810e-12  6.00000000e+00\n",
      " -9.00000000e+00  1.00000000e+00]\n",
      "\n",
      " actual minimum: [ 0 -1 -7  0  3  5  0  6 -9  1]\n",
      "\n",
      "Problem 4: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 96 | result: [ 5.00016106e+00 -5.99995237e+00 -6.99971208e+00  9.00007398e+00\n",
      " -3.00001668e+00  3.99994430e+00 -1.99974390e+00  3.99990280e+00\n",
      "  8.81652477e-05 -1.99990065e+00]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 54 | result: [ 4.99999930e+00 -6.00000035e+00 -7.00000064e+00  8.99999980e+00\n",
      " -3.00000024e+00  3.99999990e+00 -2.00000062e+00  3.99999994e+00\n",
      " -5.86627627e-07 -2.00000047e+00]\n",
      "\n",
      " actual minimum: [ 5 -6 -7  9 -3  4 -2  4  0 -2]\n",
      "\n",
      "Problem 5: \n",
      "approximated gradient: \n",
      "\n",
      "search terminated at iteration 33 | result: [-7.00001463  6.99998531 -1.00006806 -5.00014037 -2.00008892  9.00005268\n",
      " -1.99999778  6.00006372  2.99993072  7.00002436]\n",
      "\n",
      " exact gradient: \n",
      "\n",
      "search terminated at iteration 14 | result: [-7.  7. -1. -5. -2.  9. -2.  6.  3.  7.]\n",
      "\n",
      " actual minimum: [-7  7 -1 -5 -2  9 -2  6  3  7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, prob in enumerate(quadratic_probs):\n",
    "  print(f\"Problem {i+1}: \")\n",
    "  print(\"approximated gradient: \")\n",
    "  SR1(np.zeros(10), prob.f)\n",
    "  print(\"\\n exact gradient: \")\n",
    "  SR1(np.zeros(10), prob.f, prob.grad_f)\n",
    "  print(f\"\\n actual minimum: {prob.min_x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5rkOOgOcZf8"
   },
   "source": [
    "The quasi newton method is the second best method, of the four under review.\n",
    "It can solve the problems in under 100 iteration in our test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SW5-O0ZcZf8"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Testing on functions of 1-2 variables, Non-quadratic objective<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Implement functions to optimise over<br></h4><br>\n",
    "<i>Place for additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN0W_bYycZf9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxJIUhehcZf9"
   },
   "source": [
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Run 5 tests<br></h4><br>\n",
    "<p><i>Place for your additional comments and argumentation<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSEog4wqcZf9",
    "outputId": "b289e0d0-adde-469e-a883-083f3a2da361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem rosenbrock: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 22 | result: [0.9999907  0.99998138]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 24 | result: [1. 1.]\n",
      "\n",
      "actual minimum: [1 1]\n",
      "\n",
      "Problem himmelblau: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 11 | result: [2.99999999 2.        ]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 11 | result: [3. 2.]\n",
      "\n",
      "actual minimum: [[ 3.        2.      ]\n",
      " [-2.805118  3.131312]\n",
      " [-3.77931  -3.283186]\n",
      " [ 3.584428 -1.848126]]\n",
      "\n",
      "Problem poly_1: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 7 | result: [6.99999993]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 8 | result: [7.]\n",
      "\n",
      "actual minimum: [[3]\n",
      " [5]\n",
      " [7]]\n",
      "\n",
      "Problem poly_2: \n",
      "\n",
      "approximate gradient: \n",
      "\n",
      "search terminated at iteration 5 | result: [-5.34496639e-09]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 5 | result: [1.55022962e-10]\n",
      "\n",
      "actual minimum: [[ 0]\n",
      " [ 2]\n",
      " [10]]\n",
      "\n",
      "Problem poly_3: \n",
      "\n",
      "approximate gradient: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-da1c763f9de6>:39: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "search terminated at iteration 5001 | result: [nan]\n",
      "\n",
      "exact gradient: \n",
      "\n",
      "search terminated at iteration 9 | result: [9.]\n",
      "\n",
      "actual minimum: [[1]\n",
      " [6]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "prob = Problem()\n",
    "prob.rosenbrock()\n",
    "print(f\"Problem rosenbrock: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "SR1(np.array([1.2,1.2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "SR1(np.array([1.2,1.2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.himmelblau()\n",
    "print(f\"\\nProblem himmelblau: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "SR1(np.array([0,0]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "SR1(np.array([0,0]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_1()\n",
    "print(f\"\\nProblem poly_1: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "SR1(np.array([2]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "SR1(np.array([2]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_2()\n",
    "print(f\"\\nProblem poly_2: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "SR1(np.array([1]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "SR1(np.array([1]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")\n",
    "\n",
    "prob = Problem()\n",
    "prob.poly_3()\n",
    "print(f\"\\nProblem poly_3: \\n\")\n",
    "print(\"approximate gradient: \")\n",
    "SR1(np.array([7]), prob.f)\n",
    "print(\"\\nexact gradient: \")\n",
    "SR1(np.array([7]), prob.f, prob.grad_f)\n",
    "print(f\"\\nactual minimum: {prob.min_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFNIPpjxcZf9"
   },
   "source": [
    "The quasi newton method can calculate the minima of the non quadratic functions very fast. But it has problems with some of the functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oU43ESvcZf9"
   },
   "source": [
    "<h3 style=\"background-color: #D3D92B;\"><br>Template for teachers' tests<br></h3><br>\n",
    "<hr><h4 style=\"background-color: #ADB8FF;\"><br>Set up a template, how one can run your code<br></h4><br>\n",
    "Template should include sceletons for:<ul>\n",
    "    <li>custom function to optimise over </li> \n",
    "    <li>values initialisation to submit into otimising algorithm </li> \n",
    "    <li>optimiser function call</li> \n",
    "    <li>report print out call</li> </ul><br>\n",
    "Provide descriptions and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with approximate gradient:\n",
      "\n",
      "search terminated at iteration 1 | result: [-1.99687822e-09 -1.99687822e-09 -1.99687822e-09 -1.99687822e-09\n",
      " -1.99687822e-09 -1.99687822e-09 -1.99687822e-09 -1.99687822e-09\n",
      " -1.99687822e-09 -1.99687822e-09]\n",
      "\n",
      "\n",
      "\n",
      "Test with exact gradient:\n",
      "\n",
      "search terminated at iteration 1 | result: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-da1c763f9de6>:39: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mu = (sk @ sk) / (yk @ sk) - ((sk @ sk)**2 / (yk @ sk)**2 - (sk @ sk) / (yk @ yk))**0.5\n"
     ]
    }
   ],
   "source": [
    "algorithm_to_test = SR1\n",
    "\n",
    "# Here you can set your individual starting point\n",
    "x_0 = np.ones(10)\n",
    "\n",
    "# Here you can enter your individual function\n",
    "def f(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "# Here you can enter the exact gradient of your function\n",
    "# This function will just be used in the second test\n",
    "def grad_f(x):\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Test run:\n",
    "\n",
    "print(\"Test with approximate gradient:\")\n",
    "algorithm_to_test(x_0, f)\n",
    "\n",
    "print('\\n'*2) # Print some lines between the tests\n",
    "\n",
    "print(\"Test with exact gradient:\")\n",
    "algorithm_to_test(x_0, f, grad_f)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR_Xg2aOMiQJ"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf  \n",
    "[2] https://www.sciencedirect.com/science/article/pii/S0898122111004202?via%3Dihub"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "phase2_quasi_newton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
